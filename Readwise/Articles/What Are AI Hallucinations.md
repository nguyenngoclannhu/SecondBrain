# What Are AI Hallucinations?

![rw-book-cover](https://cloud.google.com/_static/cloud/images/social-icon-google-cloud-1200-630.png)

## Metadata
- Author: [[Google Cloud]]
- Full Title: What Are AI Hallucinations?
- Category: #articles
- Summary: AI hallucinations are incorrect results produced by AI models, often due to poor training data or biases. These errors can lead to serious problems, especially in critical areas like healthcare or finance. To reduce hallucinations, it's important to use high-quality data and provide clear guidance to the AI model during training.
- URL: https://cloud.google.com/discover/what-are-ai-hallucinations?hl=en

## Highlights
- AI hallucinations are incorrect or misleading results ([View Highlight](https://read.readwise.io/read/01jkda4484tpg593cmwmr7pd8q))
- including insufficient training data, incorrect assumptions made by the model, or biases in the data used to train the model. ([View Highlight](https://read.readwise.io/read/01jkda4c8kzbrjpartph5xjtt5))
    - Note: some reasons for AI hallucinations
- the accuracy of these predictions often depends on the quality and completeness of the training data ([View Highlight](https://read.readwise.io/read/01jkda5c0asb9w12nwtv4611hx))
- Regularization penalizes the model for making predictions that are too extreme. ([View Highlight](https://read.readwise.io/read/01jkdbzafhjjd15rexd631awp2))
    - Note: how?
- prevent the model from overfitting the training data and making incorrect predictions ([View Highlight](https://read.readwise.io/read/01jkdbzwcm4gcqeme4gwzgp3rd))
- When training an AI model, it is helpful to create a template for the model to follow ([View Highlight](https://read.readwise.io/read/01jkdc11x5jjgbfshatyk3av9c))
