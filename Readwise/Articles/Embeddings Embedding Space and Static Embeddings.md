# Embeddings: Embedding Space and Static Embeddings

![rw-book-cover](https://developers.google.com/static/machine-learning/crash-course/images/mlcc-hero.png)

## Metadata
- Author: [[Google for Developers]]
- Full Title: Embeddings: Embedding Space and Static Embeddings
- Category: #articles
- Summary: Embeddings represent data as vectors in a lower-dimensional space, making it easier to analyze complex information. The position of items in this space indicates their similarity, though the meanings of the dimensions can be hard to interpret. Static embeddings, like those from the word2vec model, capture semantic relationships between words based on their context in language.
- URL: https://developers.google.com/machine-learning/crash-course/embeddings/embedding-space

## Highlights
- sets the specific task and the number of embedding dimensions ([View Highlight](https://read.readwise.io/read/01jkddrk4gbzmm6j4gb99dwt9f))
- Embeddings will usually be specific to the task, and differ from each other when the task differs ([View Highlight](https://read.readwise.io/read/01jkddrcc9cjh0gkhcz99chqq3))
- When each word or data point has a single embedding vector, this is called a **static embedding** ([View Highlight](https://read.readwise.io/read/01jkddr21d4r8808xts4d734ec))
- "Efficient estimation of word representations in vector space" ([View Highlight](https://read.readwise.io/read/01jkddydsf2hxdzvq4mrjd2zez))
    - Tags: [[papers]] 
- Collapsing dimensions in this way can be misleading, because the points closest to each other in the original high-dimensional space may appear farther apart in the 3D projection. ([View Highlight](https://read.readwise.io/read/01jkde2npmakwp2gm5f48qse1c))
